{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../chap08/cnn_reg_model.ipynb\n",
    "ychn_trans=999\n",
    "ychn_conv=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnExtModel(CnnRegModel):\n",
    "    macros = {}\n",
    "    \n",
    "    def __init__(self, name, dataset, hconfigs, show_maps=False,\n",
    "                 l2_decay=0, l1_decay=0, dump_structure=False):\n",
    "        self.dump_structure = dump_structure\n",
    "        self.layer_index = 0\n",
    "        self.layer_depth = 0\n",
    "        self.param_count = 0\n",
    "        self.jongwoo = 1\n",
    "        super(CnnExtModel, self).__init__(name, dataset, hconfigs, show_maps,\n",
    "                                          l2_decay, l1_decay)\n",
    "        if self.dump_structure:\n",
    "            print('Total parameter count: {}'.format(self.param_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_layer_param(self, input_shape, hconfig):\n",
    "    layer_type = get_layer_type(hconfig)\n",
    "    \n",
    "    if layer_type in ['serial', 'parallel', 'loop', 'add', 'custom']:\n",
    "        if self.dump_structure:\n",
    "            dump_str = layer_type\n",
    "            if layer_type == 'custom':\n",
    "                name = get_conf_param(hconfig, 'name')\n",
    "                dump_str += ' ' + name\n",
    "            print('{:>{width}}{}'.format('', dump_str, \n",
    "               width=self.layer_depth*2))\n",
    "        self.layer_depth += 1;\n",
    "        \n",
    "    pm, output_shape = super(CnnExtModel, self). \\\n",
    "                          alloc_layer_param(input_shape, hconfig)\n",
    "\n",
    "    if layer_type in ['serial', 'parallel', 'loop', 'add', 'custom']:\n",
    "        self.layer_depth -= 1;\n",
    "    elif self.dump_structure:\n",
    "        self.layer_index += 1\n",
    "        pm_str = '';\n",
    "        if layer_type == 'full':\n",
    "            ph, pw = pm['w'].shape\n",
    "            pm_count = np_cpu.prod(pm['w'].shape) + pm['b'].shape[0]\n",
    "            self.param_count += pm_count\n",
    "            pm_str = ' pm:{}x{}+{}={}'.format(ph, pw, pm['b'].shape[0], pm_count)\n",
    "        elif layer_type == 'conv' or layer_type == 'trans_conv':\n",
    "            kh, kw, xchn, ychn = pm['k'].shape\n",
    "            pm_count = np_cpu.prod(pm['k'].shape) + pm['b'].shape[0]\n",
    "            self.param_count += pm_count\n",
    "            pm_str = ' pm:{}x{}x{}x{}+{}={}'.format(kh, kw, xchn, ychn,\n",
    "                                                 pm['b'].shape[0], pm_count)\n",
    "        print('{:>{width}}{}: {}, {}=>{}{}'. \\\n",
    "              format('', self.layer_index, layer_type, input_shape,\n",
    "                     output_shape, pm_str, width=self.layer_depth*2))\n",
    "\n",
    "    return pm, output_shape\n",
    "\n",
    "CnnExtModel.alloc_layer_param = cnn_ext_alloc_layer_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_parallel_layer(self, input_shape, hconfig):\n",
    "    pm_hiddens = []\n",
    "    output_shape = None\n",
    "\n",
    "    if not isinstance(hconfig[1], dict): hconfig.insert(1, {})\n",
    "        \n",
    "    for bconfig in hconfig[2:]:\n",
    "        bpm, bshape = self.alloc_layer_param(input_shape, bconfig)\n",
    "        pm_hiddens.append(bpm)\n",
    "        if output_shape:\n",
    "            assert output_shape[0:-1] == bshape[0:-1]\n",
    "            output_shape[-1] += bshape[-1]\n",
    "        else:\n",
    "            output_shape = bshape\n",
    "    \n",
    "    return {'pms':pm_hiddens}, output_shape\n",
    "    \n",
    "def cnn_ext_forward_parallel_layer(self, x, hconfig, pm):\n",
    "    bys, bauxes, bchns = [], [], []\n",
    "    for n, bconfig in enumerate(hconfig[2:]):\n",
    "        by, baux = self.forward_layer(x, bconfig, pm['pms'][n])\n",
    "        bys.append(by)\n",
    "        bauxes.append(baux)\n",
    "        bchns.append(by.shape[-1])\n",
    "    y = np.concatenate(bys, axis=-1)\n",
    "    return y, [bauxes, bchns]\n",
    "\n",
    "def cnn_ext_backprop_parallel_layer(self, G_y, hconfig, pm, aux):\n",
    "    bauxes, bchns = aux\n",
    "    bcn_from = 0\n",
    "    G_x = 0\n",
    "    for n, bconfig in enumerate(hconfig[2:]):\n",
    "        bcn_to = bcn_from + bchns[n]\n",
    "        G_y_slice = G_y[:,:,:,bcn_from:bcn_to]\n",
    "        G_x += self.backprop_layer(G_y_slice, bconfig, pm['pms'][n], bauxes[n])\n",
    "    return G_x\n",
    "\n",
    "CnnExtModel.alloc_parallel_layer = cnn_ext_alloc_parallel_layer\n",
    "CnnExtModel.forward_parallel_layer = cnn_ext_forward_parallel_layer\n",
    "CnnExtModel.backprop_parallel_layer = cnn_ext_backprop_parallel_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_serial_layer(self, input_shape, hconfig):\n",
    "    pm_hiddens = []\n",
    "    prev_shape = input_shape\n",
    "\n",
    "    if not isinstance(hconfig[1], dict): hconfig.insert(1, {})\n",
    "        \n",
    "    for sconfig in hconfig[2:]:\n",
    "        pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, sconfig)\n",
    "        pm_hiddens.append(pm_hidden)\n",
    "    \n",
    "    return {'pms':pm_hiddens}, prev_shape\n",
    "    \n",
    "def cnn_ext_forward_serial_layer(self, x, hconfig, pm):\n",
    "    hidden = x\n",
    "    auxes = []\n",
    "\n",
    "    for n, sconfig in enumerate(hconfig[2:]):\n",
    "        hidden, aux = self.forward_layer(hidden, sconfig, pm['pms'][n])\n",
    "        auxes.append(aux)\n",
    "        \n",
    "    return hidden, auxes\n",
    "\n",
    "def cnn_ext_backprop_serial_layer(self, G_y, hconfig, pm, aux):\n",
    "    auxes = aux\n",
    "    G_hidden = G_y\n",
    "    \n",
    "    for n in reversed(range(len(hconfig[2:]))):\n",
    "        sconfig, spm, saux = hconfig[2:][n], pm['pms'][n], auxes[n]\n",
    "        G_hidden = self.backprop_layer(G_hidden, sconfig, spm, saux)\n",
    "    \n",
    "    return G_hidden\n",
    "\n",
    "CnnExtModel.alloc_serial_layer = cnn_ext_alloc_serial_layer\n",
    "CnnExtModel.forward_serial_layer = cnn_ext_forward_serial_layer\n",
    "CnnExtModel.backprop_serial_layer = cnn_ext_backprop_serial_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_add_layer(self, input_shape, hconfig):\n",
    "    if not isinstance(hconfig[1], dict): hconfig.insert(1, {})\n",
    "        \n",
    "    bpm, output_shape = self.alloc_layer_param(input_shape, hconfig[2])\n",
    "    pm_hiddens = [bpm]\n",
    "\n",
    "    for bconfig in hconfig[3:]:\n",
    "        bpm, bshape = self.alloc_layer_param(input_shape, bconfig)\n",
    "        pm_hiddens.append(bpm)\n",
    "        check_add_shapes(output_shape, bshape)\n",
    "    \n",
    "    if get_conf_param(hconfig, 'x', True):\n",
    "        check_add_shapes(output_shape, input_shape)\n",
    "        \n",
    "    pm = {'pms':pm_hiddens}\n",
    "    \n",
    "    for act in get_conf_param(hconfig, 'actions', ''):\n",
    "        if act == 'B':\n",
    "            bn_config = ['batch_normal', {'rescale':True}]\n",
    "            pm['bn'], _ = self.alloc_batch_normal_layer(output_shape,\n",
    "                        bn_config)\n",
    "\n",
    "    return pm, output_shape\n",
    "\n",
    "CnnExtModel.alloc_add_layer = cnn_ext_alloc_add_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_forward_add_layer(self, x, hconfig, pm):\n",
    "    y, baux = self.forward_layer(x, hconfig[2], pm['pms'][0])\n",
    "    bauxes, bchns, aux_bn = [baux], [y.shape[-1]], []\n",
    "\n",
    "    for n, bconfig in enumerate(hconfig[3:]):\n",
    "        by, baux = self.forward_layer(x, bconfig, pm['pms'][n+1])\n",
    "        y += tile_add_result(by, y.shape[-1], by.shape[-1])\n",
    "        bauxes.append(baux)\n",
    "        bchns.append(by.shape[-1])\n",
    "\n",
    "    if get_conf_param(hconfig, 'x', True):\n",
    "        y += tile_add_result(x, y.shape[-1], x.shape[-1])\n",
    "        \n",
    "    for act in get_conf_param(hconfig, 'actions', ''):\n",
    "        if act == 'A': y = self.activate(y, hconfig)\n",
    "        if act == 'B':\n",
    "            y, aux_bn = self.forward_batch_normal_layer(y, None, pm['bn'])\n",
    "            \n",
    "    return y, [y, bauxes, bchns, aux_bn, x.shape]\n",
    "\n",
    "CnnExtModel.forward_add_layer = cnn_ext_forward_add_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_backprop_add_layer(self, G_y, hconfig, pm, aux):\n",
    "    y, bauxes, bchns, aux_bn, x_shape = aux\n",
    "\n",
    "\n",
    "    for act in reversed(get_conf_param(hconfig, 'actions', '')):\n",
    "        if act == 'A': G_y = self.activate_derv(G_y, y, hconfig)\n",
    "        if act == 'B':\n",
    "            G_y = self.backprop_batch_normal_layer(G_y, None, pm['bn'],\n",
    "                aux_bn)\n",
    "    \n",
    "    G_x = np.zeros(x_shape, dtype = 'float32')\n",
    "    \n",
    "    for n, bconfig in enumerate(hconfig[2:]):\n",
    "        G_by = merge_add_grad(G_y, G_y.shape[-1], bchns[n])\n",
    "        G_x += self.backprop_layer(G_by, bconfig, pm['pms'][n], bauxes[n])\n",
    "\n",
    "    if get_conf_param(hconfig, 'x', True):\n",
    "        G_x += merge_add_grad(G_y, G_y.shape[-1], x_shape[-1])\n",
    "\n",
    "    return G_x\n",
    "\n",
    "CnnExtModel.backprop_add_layer = cnn_ext_backprop_add_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_add_shapes(yshape, bshape):\n",
    "    assert yshape[:-1] == bshape[:-1]\n",
    "    assert yshape[-1] % bshape[-1] == 0\n",
    "    \n",
    "def tile_add_result(by, ychn, bchn):\n",
    "    if ychn == bchn: return by\n",
    "    times = ychn // bchn\n",
    "    return np.tile(by, times)\n",
    "\n",
    "def merge_add_grad(G_y, ychn, bchn):\n",
    "    if ychn == bchn: return G_y\n",
    "    times = ychn // bchn\n",
    "    split_shape = G_y.shape[:-1] + tuple([times, bchn])\n",
    "    return np.sum(G_y.reshape(split_shape), axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_loop_layer(self, input_shape, hconfig):\n",
    "    pm_hiddens = []\n",
    "    prev_shape = input_shape\n",
    " \n",
    "    if not isinstance(hconfig[1], dict): hconfig.insert(1, {})\n",
    "        \n",
    "    for n in range(get_conf_param(hconfig, 'repeat', 1)):\n",
    "        pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, hconfig[2])\n",
    "        pm_hiddens.append(pm_hidden)\n",
    "    \n",
    "    return {'pms':pm_hiddens}, prev_shape\n",
    "    \n",
    "def cnn_ext_forward_loop_layer(self, x, hconfig, pm):\n",
    "    hidden = x\n",
    "    aux_layers = []\n",
    "\n",
    "    for n in range(get_conf_param(hconfig, 'repeat', 1)):\n",
    "        hidden, aux = self.forward_layer(hidden, hconfig[2], pm['pms'][n])\n",
    "        aux_layers.append(aux)\n",
    "        \n",
    "    return hidden, aux_layers\n",
    "\n",
    "def cnn_ext_backprop_loop_layer(self, G_y, hconfig, pm, aux):\n",
    "    G_hidden = G_y\n",
    "    aux_layers = aux\n",
    "    \n",
    "    for n in reversed(range(get_conf_param(hconfig, 'repeat', 1))):\n",
    "        pm_hidden, aux = pm['pms'][n], aux_layers[n]\n",
    "        G_hidden = self.backprop_layer(G_hidden, hconfig[2], pm_hidden, aux)\n",
    "    \n",
    "    return G_hidden\n",
    "\n",
    "CnnExtModel.alloc_loop_layer = cnn_ext_alloc_loop_layer\n",
    "CnnExtModel.forward_loop_layer = cnn_ext_forward_loop_layer\n",
    "CnnExtModel.backprop_loop_layer = cnn_ext_backprop_loop_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_custom_layer(self, input_shape, hconfig):\n",
    "    name = get_conf_param(hconfig, 'name')\n",
    "    args = get_conf_param(hconfig, 'args', {})\n",
    "    macro = CnnExtModel.get_macro(name, args)\n",
    "\n",
    "    pm_hidden, output_shape = self.alloc_layer_param(input_shape, macro)\n",
    "    \n",
    "    return {'pm':pm_hidden, 'macro':macro}, output_shape\n",
    "    \n",
    "def cnn_ext_forward_custom_layer(self, x, hconfig, pm):\n",
    "    return self.forward_layer(x, pm['macro'], pm['pm'])\n",
    "\n",
    "def cnn_ext_backprop_custom_layer(self, G_y, hconfig, pm, aux):\n",
    "    return self.backprop_layer(G_y, pm['macro'], pm['pm'], aux)\n",
    "\n",
    "CnnExtModel.alloc_custom_layer = cnn_ext_alloc_custom_layer\n",
    "CnnExtModel.forward_custom_layer = cnn_ext_forward_custom_layer\n",
    "CnnExtModel.backprop_custom_layer = cnn_ext_backprop_custom_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_set_macro(name, config):\n",
    "    CnnExtModel.macros[name] = config\n",
    "    \n",
    "def cnn_ext_get_macro(name, args):\n",
    "    restored = copy.deepcopy(CnnExtModel.macros[name])\n",
    "    replace_arg(restored, args)\n",
    "        \n",
    "    return restored\n",
    "    \n",
    "def replace_arg(exp, args):\n",
    "    if isinstance(exp, (list, tuple)):\n",
    "        for n, term in enumerate(exp):\n",
    "            if isinstance(term, str) and term[0] == '#':\n",
    "                if term[1] == '#': exp[n] = term[1:]\n",
    "                elif term in args: exp[n] = args[term]\n",
    "            else:\n",
    "                replace_arg(term, args)\n",
    "    elif isinstance(exp, dict):\n",
    "        for key in exp:\n",
    "            if isinstance(exp[key], str) and exp[key][0] == '#':\n",
    "                if exp[key][1] == '#': exp[key] = exp[key][1:]\n",
    "                elif exp[key] in args: exp[key] = args[exp[key]]\n",
    "            else:\n",
    "                replace_arg(exp[key], args)\n",
    "                \n",
    "CnnExtModel.set_macro = cnn_ext_set_macro\n",
    "CnnExtModel.get_macro = cnn_ext_get_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_conv_layer(self, input_shape, hconfig):\n",
    "    pm, output_shape = super(CnnExtModel, self). \\\n",
    "                           alloc_conv_layer(input_shape, hconfig)\n",
    "    pm['actions'] = get_conf_param(hconfig, 'actions', 'LAB')\n",
    "    \n",
    "    self.layer_type=\"conv\"\n",
    "    for act in pm['actions']:\n",
    "        if act == 'L':\n",
    "            input_shape = output_shape\n",
    "        elif act == 'B':\n",
    "            bn_config = ['batch_normal', {'rescale':True}]\n",
    "            pm['bn'], _ = self.alloc_batch_normal_layer(input_shape, bn_config)\n",
    "    xh, xw, xchn = input_shape\n",
    "    ychn = get_conf_param(hconfig, 'chn')\n",
    "    output_shape = eval_stride_shape(hconfig, True, xh, xw, ychn)\n",
    "    \n",
    "    return pm, output_shape\n",
    "\n",
    "CnnExtModel.alloc_conv_layer = cnn_ext_alloc_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_forward_conv_layer(self, x, hconfig, pm):\n",
    "    y = x\n",
    "    x_flat, k_flat, relu_y, aux_bn = None, None, None, None\n",
    "    for act in pm['actions']:\n",
    "        \n",
    "        if act == 'L':\n",
    "            mb_size, xh, xw, xchn = y.shape\n",
    "            kh, kw, _, ychn = pm['k'].shape\n",
    "            \n",
    "            x_flat = get_ext_regions_for_conv(y, kh, kw)\n",
    "            k_flat = pm['k'].reshape([kh*kw*xchn, ychn])\n",
    "           \n",
    "            conv_flat = np.matmul(x_flat, k_flat)\n",
    "            y = conv_flat.reshape([mb_size, xh, xw, ychn]) + pm['b']\n",
    "        elif act == 'A':\n",
    "            y = self.activate(y, hconfig)\n",
    "            relu_y = y.copy()\n",
    "            #print(\"보조함수\",y.shape)\n",
    "        elif act == 'B':\n",
    "            y, aux_bn = self.forward_batch_normal_layer(y, None, pm['bn'])\n",
    "            #print(\"batch\",y.shape)\n",
    "        #print(\"cnn\",y.shape)\n",
    "\n",
    "    y, aux_stride = stride_filter(hconfig, True, y)\n",
    "    #print(\"stride\",y.shape)\n",
    "    if self.need_maps: self.maps.append(y)\n",
    "    #print(\"cnn\",y.shape)\n",
    "    if ychn==ychn_conv:\n",
    "       \n",
    "        y=np.sum(y,axis=3)\n",
    "        y=np.sum(y,axis=2)\n",
    "        y=np.sum(y,axis=1)\n",
    "        #print(y.shape)\n",
    "        y=y[:,np.newaxis]\n",
    "    #print(\"con\",y.shape)\n",
    "    return y, [x_flat, k_flat, x, relu_y, aux_bn, aux_stride]\n",
    "\n",
    "CnnExtModel.forward_conv_layer = cnn_ext_forward_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_backprop_conv_layer(self, G_y, hconfig, pm, aux):\n",
    "    x_flat, k_flat, x, relu_y, aux_bn, aux_stride = aux\n",
    "    \n",
    "    \n",
    "    #print(G_y.shape)\n",
    "    \n",
    "    kh, kw, xchn, ychn = pm['k'].shape\n",
    "    if ychn==ychn_conv:\n",
    "        #print(G_y.shape)\n",
    "        G_x=np.concatenate([G_y]*1,axis=1)\n",
    "        G_x=G_x[:,:,np.newaxis]\n",
    "        G_x=np.concatenate([G_x]*1,axis=2)\n",
    "        G_x=G_x[:,:,:,np.newaxis]\n",
    "        G_y=np.concatenate([G_x]*ychn,axis=3)\n",
    "        #print(G_x.shape)\n",
    "    \n",
    "    G_x = stride_filter_derv(hconfig, True, G_y, aux_stride)\n",
    "    for act in reversed(pm['actions']):\n",
    "        if act == 'L':\n",
    "            kh, kw, xchn, ychn = pm['k'].shape\n",
    "            mb_size, xh, xw, _ = G_x.shape\n",
    "\n",
    "            G_conv_flat = G_x.reshape(mb_size*xh*xw, ychn)\n",
    "            g_conv_k_flat = x_flat.transpose()\n",
    "            g_conv_x_flat = k_flat.transpose()\n",
    "            G_k_flat = np.matmul(g_conv_k_flat, G_conv_flat)\n",
    "            G_x_flat = np.matmul(G_conv_flat, g_conv_x_flat)\n",
    "            G_bias = np.sum(G_conv_flat, axis=0)\n",
    "            G_kernel = G_k_flat.reshape([kh, kw, xchn, ychn])\n",
    "            G_x = undo_ext_regions_for_conv(G_x_flat, x, kh, kw)\n",
    "            \n",
    "            self.update_param(pm, 'k', G_kernel)\n",
    "            self.update_param(pm, 'b', G_bias)\n",
    "        elif act == 'A':\n",
    "            G_x = self.activate_derv(G_x, relu_y, hconfig)\n",
    "        elif act == 'B':\n",
    "            G_x = self.backprop_batch_normal_layer(G_x, None, pm['bn'], aux_bn)\n",
    "    \n",
    "    return G_x\n",
    "    \n",
    "CnnExtModel.backprop_conv_layer = cnn_ext_backprop_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_trans_conv_layer(self, input_shape, hconfig):\n",
    "    pm, output_shape = super(CnnExtModel, self). \\\n",
    "                           alloc_conv_layer(input_shape, hconfig)\n",
    "    xh, xw, xchn = input_shape\n",
    "    ychn = get_conf_param(hconfig, 'chn')\n",
    "    output_shape = trans_eval_stride_shape(hconfig, True, xh, xw, ychn)\n",
    "    pm['actions'] = get_conf_param(hconfig, 'actions', 'LAB')\n",
    "    self.layer_type=\"trans_conv\"\n",
    "    for act in pm['actions']:\n",
    "        #print(act)\n",
    "        if act == 'L':\n",
    "            pass\n",
    "        #xh+kh-2+sh\n",
    "        elif act == 'B':\n",
    "            bn_config = ['batch_normal', {'rescale':True}]\n",
    "            pm['bn'], _ = self.alloc_batch_normal_layer(output_shape, bn_config)\n",
    "\n",
    "    #output_shape = trans_eval_stride_shape(hconfig, True, xh, xw, ychn)\n",
    "    \n",
    "    return pm, output_shape\n",
    "    #print(xh+pm['k'].shape[0],xw+pm['k'].shape[1])\n",
    "    #return pm, [xh+pm['k'].shape[0]-1,xw+pm['k'].shape[1]-1,ychn]\n",
    "\n",
    "CnnExtModel.alloc_trans_conv_layer = cnn_ext_alloc_trans_conv_layer\n",
    "\n",
    "def cnn_ext_forward_trans_conv_layer(self, x, hconfig, pm):\n",
    "    _, _, sh, sw, _ = get_shape_params(hconfig, True)\n",
    "    y = x\n",
    "    x_flat, k_flat, relu_y, aux_bn = None, None, None, None\n",
    "    for act in pm['actions']:\n",
    "        if act == 'L':\n",
    "            \"\"\"\n",
    "            mb_size, xh, xw, xchn = y.shape\n",
    "            kh, kw, _, ychn = pm['k'].shape\n",
    "            temp = np.zeros((mb_size,xh+kh-1, xw+kw-1,ychn))\n",
    "           \n",
    "            print(\"y\",y[0, :,:,0],y[0, :,:,1],y[0, :,:,2])\n",
    "            for p in range(mb_size):\n",
    "                for l in range(xchn):\n",
    "                    for i in range(xh):\n",
    "                        for j in range(xw):\n",
    "                            for k in range(ychn):\n",
    "                                temp[p,i:i+kh, j:j+kw,k] += y[p, i,j,l] * pm['k'][:,:,l,k]\n",
    "            \n",
    "            for p in range(mb_size): #1\n",
    "                for l in range(xchn):\n",
    "                    for i in range(xh): #1\n",
    "                        for j in range(xw): #2\n",
    "                            for k in range(ychn): #2\n",
    "                                for t in range(kh): #1\n",
    "                                    for o in range(kw): #2\n",
    "                                        temp[p,i+t, j+o,k] += y[p, i,j,l] * pm['k'][t,o,l,k]\n",
    "            print(\"temp\",temp[0,:,:,0].astype(int),temp[1,:,:,0].astype(int))\n",
    "            \"\"\"\n",
    "            \n",
    "            #----------------------------------------\n",
    "            #print(y.shape)\n",
    "            mb_size, xh, xw, xchn = y.shape\n",
    "            kh, kw, _, ychn = pm['k'].shape\n",
    "            temp = np.zeros((mb_size,sh*(xh-1) + kh, sw*(xw-1) + kw,ychn), dtype = 'float32') #결과물 mb_size,xh+kh-1, xw,xchn    kh,kw,xchn ychn ,xw+kw-1,ychn\n",
    "            x_flat=y.reshape((mb_size*xh*xw,xchn))\n",
    "            k_flat=pm['k'].transpose((2,0,1,3)).reshape((xchn,kh*kw*ychn))\n",
    "            temp_=np.matmul(x_flat, k_flat)\n",
    "            temp_=temp_.reshape((mb_size,xh,xw,kh,kw,ychn))\n",
    "            #print(\"Tt\")\n",
    "            for i in range(xh):\n",
    "                \n",
    "                temp_i=sh*i\n",
    "                for j in range(xw):\n",
    "                    temp_j=sw*j\n",
    "                   \n",
    "         \n",
    "                    temp[:,temp_i:temp_i+kh,temp_j:temp_j+kw,:]+=temp_[:,i,j,:,:,:]\n",
    "                    \n",
    "            y=temp+ pm['b']\n",
    "            #print(\"trans\",y.shape)\n",
    "            #print(1)\n",
    "        elif act == 'A':\n",
    "            y = self.activate(y, hconfig)\n",
    "            relu_y = y.copy()\n",
    "            #print(2)\n",
    "        elif act == 'B':\n",
    "            y, aux_bn = self.forward_batch_normal_layer(y, None, pm['bn'])\n",
    "            #print(3)\n",
    "    if ychn==ychn_trans:\n",
    "        #print(G_y.shape)\n",
    "        y_1=y[:,:,:,:int(ychn_trans/3)]\n",
    "        y_1=np.sum(y_1,axis=3)\n",
    "        y_1=y_1[:,:,:,np.newaxis]\n",
    "        y_2=y[:,:,:,int(ychn_trans/3):int(2*ychn_trans/3)]\n",
    "        y_2=np.sum(y_2,axis=3)\n",
    "        y_2=y_2[:,:,:,np.newaxis]\n",
    "        y_3=y[:,:,:,int(2*ychn_trans/3):ychn_trans]\n",
    "        y_3=np.sum(y_3,axis=3)\n",
    "        y_3=y_3[:,:,:,np.newaxis]\n",
    "        y=np.concatenate((y_1,y_2,y_3),axis=3)\n",
    "        \n",
    "    #print(y.shape)\n",
    "    if self.need_maps: self.maps.append(y)\n",
    "    #print(aux_bn.shape)\n",
    "    return y, [x_flat, k_flat, x, relu_y, aux_bn, sh, sw]\n",
    "\n",
    "CnnExtModel.forward_trans_conv_layer = cnn_ext_forward_trans_conv_layer\n",
    "\n",
    "def cnn_ext_backprop_trans_conv_layer(self, G_y, hconfig, pm, aux):\n",
    "    x_flat, k_flat, x, relu_y, aux_bn , sh, sw = aux\n",
    "    \n",
    "    G_x = G_y\n",
    "    kh, kw, xchn, ychn = pm['k'].shape\n",
    "    \n",
    "    if ychn==ychn_trans:\n",
    "        G_x_1=G_x[:,:,:,0]\n",
    "        G_x_1=np.concatenate([G_x_1[:,:,:,np.newaxis]]*int(ychn_trans/3),axis=3)\n",
    "        G_x_2=G_x[:,:,:,1]\n",
    "        G_x_2=np.concatenate([G_x_2[:,:,:,np.newaxis]]*int(ychn_trans/3),axis=3)\n",
    "        G_x_3=G_x[:,:,:,2]\n",
    "        G_x_3=np.concatenate([G_x_3[:,:,:,np.newaxis]]*int(ychn_trans/3),axis=3)\n",
    "        #print(G_x_1.shape,G_x_2.shape,G_x_3.shape)\n",
    "        G_x=np.concatenate((G_x_1,G_x_2,G_x_3),axis=3)\n",
    "        #print(G_x.shape)\n",
    "    #print(G_x.shape)\n",
    "    \n",
    "    for act in reversed(pm['actions']):\n",
    "        if act == 'L':\n",
    "            kh, kw, xchn, ychn = pm['k'].shape\n",
    "            mb_size, a_,b_, _ = G_x.shape\n",
    "            #sw*(xw-1) + kw swxw - sw + kw = a_ -kw +sw / sw\n",
    "            xh=int((a_-kh+sh)/sh)\n",
    "            xw=int((b_-kw+sw)/sw)\n",
    "            #print(xh,xw)\n",
    "            temp = np.zeros((mb_size,xh,xw,kh,kw,ychn), dtype = 'float32')\n",
    "            #print(temp.shape)\n",
    "            #print(x_flat.transpose().shape)#mb_size*xh*xw,xchn.T\n",
    "            #print(k_flat.transpose().shape) #xchn,kh*kw*ychn.T\n",
    "            for i in range(xh):\n",
    "                \n",
    "                temp_i=sh*i\n",
    "                for j in range(xw):\n",
    "                    \n",
    "                    temp_j=sw*j\n",
    "                    temp[:,i,j,:,:,:]+=G_x[:,temp_i:temp_i+kh,temp_j:temp_j+kw,:] #temp.shape(mb_size,xh,xw,kh,kw,ychn) G_x.shape(mb_size,xh+kh-1,xw+kw-1,ychn), \n",
    "            #print(\"11\",G_x[:,:,:,0])\n",
    "            G_conv_flat = temp.reshape(mb_size*xh*xw, kh*kw*ychn)\n",
    "            #print(\"22\",temp[:,:,:,:,:,0])\n",
    "            #print(\"33\",G_conv_flat.shape)\n",
    "            #print(G_conv_flat)\n",
    "            g_conv_k_flat = x_flat.transpose() #xchn,mb_size*xh*xw  mb_size*xh*xw, kh*kw*ychn    xchn,kh*kw*ychn\n",
    "            g_conv_x_flat = k_flat.transpose() #mb_size*xh*xw, kh*kw*ychn  kh*kw*ychn,xchn    mb_size*xh*xw,xchn\n",
    "            #print(\"44\",g_conv_k_flat.shape)\n",
    "            #print(g_conv_k_flat)\n",
    "            #print(\"55\",g_conv_x_flat.shape)\n",
    "            #print(g_conv_x_flat)\n",
    "            G_k_flat = np.matmul(g_conv_k_flat, G_conv_flat) #xchn,mb_size*xh*xw\n",
    "            G_x_flat = np.matmul(G_conv_flat, g_conv_x_flat) #mb_size*xh*xw,xchn\n",
    "            G_bias = np.sum(G_conv_flat.reshape(-1,ychn), axis=0)\n",
    "            G_k_flat=G_k_flat.reshape([xchn,kh,kw,ychn])\n",
    "            G_kernel = G_k_flat.transpose((1,2,0,3))\n",
    "            G_x=G_x_flat.reshape((mb_size,xh,xw,xchn))\n",
    "            #print(G_kernel[0].dtype)\n",
    "            #print(\"ttwetw\",pm['k'][0].dtype)\n",
    "            \n",
    "                    \n",
    "            #print(G_kernel.shape)\n",
    "            self.update_param(pm, 'k', G_kernel)\n",
    "            self.update_param(pm, 'b', G_bias)\n",
    "        elif act == 'A':\n",
    "            G_x = self.activate_derv(G_x, relu_y, hconfig)\n",
    "        elif act == 'B':\n",
    "            G_x = self.backprop_batch_normal_layer(G_x, None, pm['bn'], aux_bn)\n",
    "\n",
    "    return G_x\n",
    "    \n",
    "CnnExtModel.backprop_trans_conv_layer = cnn_ext_backprop_trans_conv_layer\n",
    "\n",
    "def trans_eval_stride_shape(hconfig, conv_type, xh, xw, ychn):\n",
    "    kh, kw, sh, sw, padding = get_shape_params(hconfig, conv_type)\n",
    "    #if padding == 'VALID':\n",
    "    yh = sh*(xh-1) + kh #sh xh kh   2 3 2  6  2 2 2 4  3 3 2 8  3 3 3\n",
    "    yw = sw*(xw-1) + kw\n",
    "\n",
    "    return [yh, yw, ychn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jongwoo_cnn_ext_alloc_trans_conv_layer(self, input_shape, hconfig):\n",
    "    pm, output_shape = super(CnnExtModel, self). \\\n",
    "                           alloc_conv_layer(input_shape, hconfig)\n",
    "    xh, xw, xchn = input_shape\n",
    "    ychn = get_conf_param(hconfig, 'chn')\n",
    "    \n",
    "    output_shape = jongwoo_trans_eval_stride_shape(hconfig, True, xh, xw, ychn)\n",
    "    #print(output_shape)\n",
    "    magnitude = np.random.normal(1, self.rand_std, output_shape).astype(np.float32)\n",
    "    pm.setdefault('magnitude',magnitude)\n",
    "    \n",
    "    pm['actions'] = get_conf_param(hconfig, 'actions', 'LAB')\n",
    "    self.layer_type=\"trans_conv\"\n",
    "    for act in pm['actions']:\n",
    "        #print(act)\n",
    "        if act == 'L':\n",
    "            pass\n",
    "        #xh+kh-2+sh\n",
    "        elif act == 'B':\n",
    "            bn_config = ['batch_normal', {'rescale':True}]\n",
    "            pm['bn'], _ = self.alloc_batch_normal_layer(output_shape, bn_config)\n",
    "\n",
    "    #output_shape = trans_eval_stride_shape(hconfig, True, xh, xw, ychn)\n",
    "    \n",
    "    return pm, output_shape\n",
    "    #print(xh+pm['k'].shape[0],xw+pm['k'].shape[1])\n",
    "    #return pm, [xh+pm['k'].shape[0]-1,xw+pm['k'].shape[1]-1,ychn]\n",
    "\n",
    "CnnExtModel.alloc_jongwoo_trans_conv_layer = jongwoo_cnn_ext_alloc_trans_conv_layer\n",
    "\n",
    "def jongwoo_cnn_ext_forward_trans_conv_layer(self, x, hconfig, pm):\n",
    "    _, _, sh, sw, _ = get_shape_params(hconfig, True)\n",
    "    y = x\n",
    "    x_flat, k_flat, relu_y, aux_bn = None, None, None, None\n",
    "    for act in pm['actions']:\n",
    "        if act == 'L':\n",
    "            \"\"\"\n",
    "            mb_size, xh, xw, xchn = y.shape\n",
    "            kh, kw, _, ychn = pm['k'].shape\n",
    "            temp = np.zeros((mb_size,xh+kh-1, xw+kw-1,ychn))\n",
    "           \n",
    "            print(\"y\",y[0, :,:,0],y[0, :,:,1],y[0, :,:,2])\n",
    "            for p in range(mb_size):\n",
    "                for l in range(xchn):\n",
    "                    for i in range(xh):\n",
    "                        for j in range(xw):\n",
    "                            for k in range(ychn):\n",
    "                                temp[p,i:i+kh, j:j+kw,k] += y[p, i,j,l] * pm['k'][:,:,l,k]\n",
    "            \n",
    "            for p in range(mb_size): #1\n",
    "                for l in range(xchn):\n",
    "                    for i in range(xh): #1\n",
    "                        for j in range(xw): #2\n",
    "                            for k in range(ychn): #2\n",
    "                                for t in range(kh): #1\n",
    "                                    for o in range(kw): #2\n",
    "                                        temp[p,i+t, j+o,k] += y[p, i,j,l] * pm['k'][t,o,l,k]\n",
    "            print(\"temp\",temp[0,:,:,0].astype(int),temp[1,:,:,0].astype(int))\n",
    "            \"\"\"\n",
    "            \n",
    "            #----------------------------------------\n",
    "            #print(y.shape)\n",
    "            mb_size, xh, xw, xchn = y.shape\n",
    "            kh, kw, _, ychn = pm['k'].shape\n",
    "            temp = np.zeros((mb_size,sh*(xh-1) + kh, sw*(xw-1) + kw,ychn), dtype = 'float32') #결과물 mb_size,xh+kh-1, xw,xchn    kh,kw,xchn ychn ,xw+kw-1,ychn\n",
    "            #temp_jongwoo = np.zeros((mb_size,sh*(xh-1) + kh, sw*(xw-1) + kw,ychn), dtype = 'float32')\n",
    "            x_flat=y.reshape((mb_size*xh*xw,xchn))\n",
    "            k_flat=pm['k'].transpose((2,0,1,3)).reshape((xchn,kh*kw*ychn))\n",
    "            temp_=np.matmul(x_flat, k_flat)\n",
    "            temp_=temp_.reshape((mb_size,xh,xw,kh,kw,ychn))\n",
    "            #temp_jongwoo_patch=np.ones((mb_size,xh,xw,kh,kw,ychn))\n",
    "            #print(\"Tt\")\n",
    "            for i in range(xh):\n",
    "                \n",
    "                temp_i=sh*i\n",
    "                for j in range(xw):\n",
    "                    temp_j=sw*j\n",
    "                   \n",
    "         \n",
    "                    temp[:,temp_i:temp_i+kh,temp_j:temp_j+kw,:]+=temp_[:,i,j,:,:,:]\n",
    "                    #temp_jongwoo[:,temp_i:temp_i+kh,temp_j:temp_j+kw,:]+=temp_jongwoo_patch[:,i,j,:,:,:]\n",
    "                    \n",
    "            #temp=temp/temp_jongwoo\n",
    "            temp=temp*pm['magnitude']\n",
    "            #print(pm['magnitude'][0,0,0])\n",
    "            y=temp+ pm['b']\n",
    "            #print(\"trans\",y.shape)\n",
    "            #print(1)\n",
    "        elif act == 'A':\n",
    "            y = self.activate(y, hconfig)\n",
    "            relu_y = y.copy()\n",
    "            #print(2)\n",
    "        elif act == 'B':\n",
    "            y, aux_bn = self.forward_batch_normal_layer(y, None, pm['bn'])\n",
    "            #print(3)\n",
    "    if ychn==ychn_trans:\n",
    "        #print(G_y.shape)\n",
    "        y_1=y[:,:,:,:int(ychn_trans/3)]\n",
    "        y_1=np.sum(y_1,axis=3)\n",
    "        y_1=y_1[:,:,:,np.newaxis]\n",
    "        y_2=y[:,:,:,int(ychn_trans/3):int(2*ychn_trans/3)]\n",
    "        y_2=np.sum(y_2,axis=3)\n",
    "        y_2=y_2[:,:,:,np.newaxis]\n",
    "        y_3=y[:,:,:,int(2*ychn_trans/3):ychn_trans]\n",
    "        y_3=np.sum(y_3,axis=3)\n",
    "        y_3=y_3[:,:,:,np.newaxis]\n",
    "        y=np.concatenate((y_1,y_2,y_3),axis=3)\n",
    "    #if self.epoch %10 == 0 and ychn==3:\n",
    "        #print(pm['magnitude'])\n",
    "    #print(y.shape)\n",
    "    if self.need_maps: self.maps.append(y)\n",
    "    #print(aux_bn.shape)\n",
    "    return y, [x_flat, k_flat, x, relu_y, aux_bn, sh, sw]\n",
    "CnnExtModel.forward_jongwoo_trans_conv_layer = jongwoo_cnn_ext_forward_trans_conv_layer\n",
    "\n",
    "def jongwoo_cnn_ext_backprop_trans_conv_layer(self, G_y, hconfig, pm, aux):\n",
    "    x_flat, k_flat, x, relu_y, aux_bn , sh, sw = aux\n",
    "    \n",
    "    G_x = G_y\n",
    "    kh, kw, xchn, ychn = pm['k'].shape\n",
    "    \n",
    "    if ychn==ychn_trans:\n",
    "        G_x_1=G_x[:,:,:,0]\n",
    "        G_x_1=np.concatenate([G_x_1[:,:,:,np.newaxis]]*int(ychn_trans/3),axis=3)\n",
    "        G_x_2=G_x[:,:,:,1]\n",
    "        G_x_2=np.concatenate([G_x_2[:,:,:,np.newaxis]]*int(ychn_trans/3),axis=3)\n",
    "        G_x_3=G_x[:,:,:,2]\n",
    "        G_x_3=np.concatenate([G_x_3[:,:,:,np.newaxis]]*int(ychn_trans/3),axis=3)\n",
    "        #print(G_x_1.shape,G_x_2.shape,G_x_3.shape)\n",
    "        G_x=np.concatenate((G_x_1,G_x_2,G_x_3),axis=3)\n",
    "        #print(G_x.shape)\n",
    "    #print(G_x.shape)\n",
    "    \n",
    "    for act in reversed(pm['actions']):\n",
    "        if act == 'L':\n",
    "            kh, kw, xchn, ychn = pm['k'].shape\n",
    "            mb_size, a_,b_, _ = G_x.shape\n",
    "            #sw*(xw-1) + kw swxw - sw + kw = a_ -kw +sw / sw\n",
    "            xh=int((a_-kh+sh)/sh)\n",
    "            xw=int((b_-kw+sw)/sw)\n",
    "            #print(xh,xw)\n",
    "            temp = np.zeros((mb_size,xh,xw,kh,kw,ychn), dtype = 'float32')\n",
    "            #print(temp.shape)\n",
    "            #G_x=G_x/temp_jongwoo\n",
    "            #G_x = np.sum(G_x, axis=0)\n",
    "            #print(pm['magnitude'].shape)\n",
    "            G_magnitude=np.sum(G_x,axis=0)*pm['magnitude']\n",
    "            G_x=G_x*pm['magnitude']\n",
    "            #print(G_x.shape)\n",
    "            \n",
    "            for i in range(xh):\n",
    "                \n",
    "                temp_i=sh*i\n",
    "                for j in range(xw):\n",
    "                    \n",
    "                    temp_j=sw*j\n",
    "                    temp[:,i,j,:,:,:]+=G_x[:,temp_i:temp_i+kh,temp_j:temp_j+kw,:] #temp.shape(mb_size,xh,xw,kh,kw,ychn) G_x.shape(mb_size,xh+kh-1,xw+kw-1,ychn), \n",
    "            #print(\"11\",G_x[:,:,:,0])\n",
    "            G_conv_flat = temp.reshape(mb_size*xh*xw, kh*kw*ychn)\n",
    "            #print(\"22\",temp[:,:,:,:,:,0])\n",
    "            #print(\"33\",G_conv_flat.shape)\n",
    "            #print(G_conv_flat)\n",
    "            g_conv_k_flat = x_flat.transpose() #xchn,mb_size*xh*xw  mb_size*xh*xw, kh*kw*ychn    xchn,kh*kw*ychn\n",
    "            g_conv_x_flat = k_flat.transpose() #mb_size*xh*xw, kh*kw*ychn  kh*kw*ychn,xchn    mb_size*xh*xw,xchn\n",
    "            #print(\"44\",g_conv_k_flat.shape)\n",
    "            #print(g_conv_k_flat)\n",
    "            #print(\"55\",g_conv_x_flat.shape)\n",
    "            #print(g_conv_x_flat)\n",
    "            G_k_flat = np.matmul(g_conv_k_flat, G_conv_flat) #xchn,mb_size*xh*xw\n",
    "            G_x_flat = np.matmul(G_conv_flat, g_conv_x_flat) #mb_size*xh*xw,xchn\n",
    "            G_bias = np.sum(G_conv_flat.reshape(-1,ychn), axis=0)\n",
    "            G_k_flat=G_k_flat.reshape([xchn,kh,kw,ychn])\n",
    "            G_kernel = G_k_flat.transpose((1,2,0,3))\n",
    "            G_x=G_x_flat.reshape((mb_size,xh,xw,xchn))\n",
    "            #print(G_kernel[0].dtype)\n",
    "            #print(\"ttwetw\",pm['k'][0].dtype)\n",
    "            self.learning_rate*=1\n",
    "            self.update_param(pm, 'magnitude', G_magnitude)\n",
    "            #print(x_flat.transpose().shape)#mb_size*xh*xw,xchn.T\n",
    "            #print(k_flat.transpose().shape) #xchn,kh*kw*ychn.T\n",
    "            self.learning_rate/=1\n",
    "                    \n",
    "            #print(G_kernel.shape)\n",
    "            self.update_param(pm, 'k', G_kernel)\n",
    "            self.update_param(pm, 'b', G_bias)\n",
    "        elif act == 'A':\n",
    "            G_x = self.activate_derv(G_x, relu_y, hconfig)\n",
    "        elif act == 'B':\n",
    "            G_x = self.backprop_batch_normal_layer(G_x, None, pm['bn'], aux_bn)\n",
    "\n",
    "    return G_x\n",
    "    \n",
    "CnnExtModel.backprop_jongwoo_trans_conv_layer = jongwoo_cnn_ext_backprop_trans_conv_layer\n",
    "\n",
    "def jongwoo_trans_eval_stride_shape(hconfig, conv_type, xh, xw, ychn):\n",
    "    kh, kw, sh, sw, padding = get_shape_params(hconfig, conv_type)\n",
    "    #if padding == 'VALID':\n",
    "    yh = sh*(xh-1) + kh #sh xh kh   2 3 2  6  2 2 2 4  3 3 2 8  3 3 3\n",
    "    yw = sw*(xw-1) + kw\n",
    "\n",
    "    return [yh, yw, ychn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jongwoo_no_cnn_ext_alloc_trans_conv_layer(self, input_shape, hconfig):\n",
    "    \n",
    "    xh, xw, xchn = input_shape\n",
    "    \n",
    "    \n",
    "    magnitude = np.random.normal(1, self.rand_std, input_shape).astype(np.float32)\n",
    "    pm={'magnitude':magnitude}\n",
    "    \n",
    "    \n",
    "    return pm, input_shape\n",
    "    #print(xh+pm['k'].shape[0],xw+pm['k'].shape[1])\n",
    "    #return pm, [xh+pm['k'].shape[0]-1,xw+pm['k'].shape[1]-1,ychn]\n",
    "\n",
    "CnnExtModel.alloc_no_trans_conv_layer = jongwoo_no_cnn_ext_alloc_trans_conv_layer\n",
    "\n",
    "def jongwoo_no_cnn_ext_forward_trans_conv_layer(self, x, hconfig, pm):\n",
    "    y = x\n",
    "    \n",
    "    y=y*pm['magnitude']\n",
    "    y = self.activate(y, hconfig)\n",
    "            \n",
    "    #print(pm['magnitude'][0,0,0])\n",
    "    #y=temp+ pm['b']\n",
    "    #print(\"trans\",y.shape)\n",
    "    #print(1)\n",
    "\n",
    "    return y, [x, y]\n",
    "CnnExtModel.forward_no_trans_conv_layer = jongwoo_no_cnn_ext_forward_trans_conv_layer\n",
    "\n",
    "def jongwoo_no_cnn_ext_backprop_trans_conv_layer(self, G_y, hconfig, pm, aux):\n",
    "    x, y = aux\n",
    "    \n",
    "    #G_x = G_y\n",
    "    G_y = self.activate_derv(G_y, y, hconfig)\n",
    "    \n",
    "    G_magnitude = np.sum(G_y,axis=0)*pm['magnitude']\n",
    "    self.learning_rate*=1\n",
    "    G_y *= pm['magnitude']\n",
    "    #self.learning_rate/=1000\n",
    "    self.update_param(pm, 'magnitude', G_magnitude)\n",
    "    self.learning_rate/=1\n",
    "    #self.update_param(pm, 'b', G_bias)\n",
    "    \n",
    "    return G_y\n",
    "    \n",
    "CnnExtModel.backprop_no_trans_conv_layer = jongwoo_no_cnn_ext_backprop_trans_conv_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_max_layer(self, input_shape, hconfig):\n",
    "    xh, xw, ychn = input_shape\n",
    "    output_shape = eval_stride_shape(hconfig, False, xh, xw, ychn)\n",
    "    return None, output_shape \n",
    "\n",
    "CnnExtModel.alloc_max_layer = cnn_ext_alloc_max_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_forward_max_layer(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, chn = x.shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride', [1,1])\n",
    "    kh, kw = get_conf_param_2d(hconfig, 'ksize', [sh, sw])\n",
    "    #print( sh, sw, kh, kw)\n",
    "    padding = get_conf_param(hconfig, 'padding', 'SAME')\n",
    "\n",
    "    if [sh,sw] == [kh, kw] and xh % sh == 0 and xw % sw == 0 and \\\n",
    "        padding == 'SAME':\n",
    "        return super(CnnExtModel, self).forward_max_layer(x, hconfig, pm)\n",
    "\n",
    "    x_flat = get_ext_regions(x, kh, kw, -np.inf)\n",
    "    x_flat = x_flat.transpose([2,5,0,1,3,4])\n",
    "    x_flat = x_flat.reshape(mb_size*chn*xh*xw,kh*kw)\n",
    "\n",
    "    max_idx = np.argmax(x_flat, axis=1)\n",
    "    y = x_flat[np.arange(x_flat.shape[0]), max_idx]\n",
    "    y = y.reshape([mb_size, chn, xh, xw])\n",
    "    y = y.transpose([0,2,3,1])\n",
    "\n",
    "    y, aux_stride = stride_filter(hconfig, False, y)\n",
    "    \n",
    "    if self.need_maps: self.maps.append(y)\n",
    "        \n",
    "    return y, [x.shape, kh, kw, sh, sw, padding, max_idx, aux_stride]\n",
    "\n",
    "CnnExtModel.forward_max_layer = cnn_ext_forward_max_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_backprop_max_layer(self, G_y, hconfig, pm, aux):\n",
    "    if not isinstance(aux, list):\n",
    "        return super(CnnExtModel, self).backprop_max_layer(G_y, hconfig,\n",
    "                pm, aux)\n",
    "    \n",
    "    x_shape, kh, kw, sh, sw, padding, max_idx, aux_stride = aux\n",
    "    mb_size, xh, xw, chn = x_shape\n",
    "    \n",
    "    G_y = stride_filter_derv(hconfig, False, G_y, aux_stride)\n",
    "    \n",
    "    G_y = G_y.transpose([0,3,1,2])\n",
    "    G_y = G_y.flatten()\n",
    "    \n",
    "    G_x_flat = np.zeros([mb_size*chn*xh*xw, kh*kw], dtype = 'float32')\n",
    "    G_x_flat[np.arange(G_x_flat.shape[0]), max_idx] = G_y\n",
    "\n",
    "    G_x_flat = G_x_flat.reshape(mb_size, chn, xh, xw, kh, kw)\n",
    "    G_x_flat = G_x_flat.transpose([2,3,0,4,5,1])\n",
    "    G_x = undo_ext_regions(G_x_flat, kh, kw)\n",
    "\n",
    "    return G_x\n",
    "\n",
    "CnnExtModel.backprop_max_layer = cnn_ext_backprop_max_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_alloc_avg_layer(self, input_shape, hconfig):\n",
    "    xh, xw, chn = input_shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride', [1,1])\n",
    "    kh, kw = get_conf_param_2d(hconfig, 'ksize', [sh, sw])\n",
    "    padding = get_conf_param(hconfig, 'padding', 'SAME')\n",
    "\n",
    "    if [sh,sw] == [kh, kw] and xh % sh == 0 and xw % sw == 0 \\\n",
    "                   and padding == 'SAME':\n",
    "        return super(CnnExtModel, self).alloc_avg_layer(input_shape, hconfig)\n",
    "\n",
    "    one_mask = np.ones([1, xh, xw, chn], dtype = 'float32')\n",
    "    \n",
    "    m_flat = get_ext_regions(one_mask, kh, kw, 0)\n",
    "    m_flat = m_flat.transpose([2,5,0,1,3,4])\n",
    "    m_flat = m_flat.reshape(1*chn*xh*xw,kh*kw)\n",
    "\n",
    "    mask = np.sum(m_flat, axis=1)\n",
    "    \n",
    "    output_shape = eval_stride_shape(hconfig, False, xh, xw, chn)\n",
    "\n",
    "    return {'mask':mask}, output_shape \n",
    "\n",
    "CnnExtModel.alloc_avg_layer = cnn_ext_alloc_avg_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_forward_avg_layer(self, x, hconfig, pm):\n",
    "    mb_size, xh, xw, chn = x.shape\n",
    "    sh, sw = get_conf_param_2d(hconfig, 'stride', [1,1])\n",
    "    kh, kw = get_conf_param_2d(hconfig, 'ksize', [sh, sw])\n",
    "    padding = get_conf_param(hconfig, 'padding', 'SAME')\n",
    "\n",
    "    if [sh,sw] == [kh, kw] and xh % sh == 0 and xw % sw == 0 \\\n",
    "                   and padding == 'SAME':\n",
    "        return super(CnnExtModel, self).forward_avg_layer(x, hconfig, pm)\n",
    "\n",
    "    x_flat = get_ext_regions(x, kh, kw, 0)\n",
    "    x_flat = x_flat.transpose([2,5,0,1,3,4])\n",
    "    x_flat = x_flat.reshape(mb_size*chn*xh*xw,kh*kw)\n",
    "\n",
    "    hap = np.sum(x_flat, axis=1)\n",
    "    \n",
    "    y = np.reshape(hap, [mb_size, -1]) / pm['mask']\n",
    "    y = y.reshape([mb_size, chn, xh, xw])\n",
    "    y = y.transpose([0,2,3,1])\n",
    "\n",
    "    y, aux_stride = stride_filter(hconfig, False, y)\n",
    "    \n",
    "    if self.need_maps: self.maps.append(y)\n",
    "        \n",
    "    return y, [x.shape, kh, kw, sh, sw, padding, aux_stride]\n",
    "\n",
    "CnnExtModel.forward_avg_layer = cnn_ext_forward_avg_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_ext_backprop_avg_layer(self, G_y, hconfig, pm, aux):\n",
    "    if not isinstance(aux, list):\n",
    "        return super(CnnExtModel, self).backprop_avg_layer(G_y, hconfig, pm, aux)\n",
    "    \n",
    "    x_shape, kh, kw, sh, sw, padding, aux_stride = aux\n",
    "    mb_size, xh, xw, chn = x_shape\n",
    "    \n",
    "    G_y = stride_filter_derv(hconfig, False, G_y, aux_stride)\n",
    "    \n",
    "    G_y = G_y.transpose([0,3,1,2])\n",
    "    G_y = G_y.flatten()\n",
    "    \n",
    "    G_hap = np.reshape(G_y, [mb_size, -1]) / pm['mask']\n",
    "    G_x_flat = np.tile(G_hap, (kh*kw, 1))\n",
    "\n",
    "    G_x_flat = G_x_flat.reshape(mb_size, chn, xh, xw, kh, kw)\n",
    "    G_x_flat = G_x_flat.transpose([2,3,0,4,5,1])\n",
    "    G_x = undo_ext_regions(G_x_flat, kh, kw)\n",
    "\n",
    "    return G_x\n",
    "\n",
    "CnnExtModel.backprop_avg_layer = cnn_ext_backprop_avg_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_stride_shape(hconfig, conv_type, xh, xw, ychn):\n",
    "    kh, kw, sh, sw, padding = get_shape_params(hconfig, conv_type)\n",
    "    if padding == 'VALID':\n",
    "        xh = xh - kh + 1\n",
    "        xw = xw - kw + 1\n",
    "    yh = xh // sh\n",
    "    yw = xw // sw\n",
    "    return [yh, yw, ychn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_filter(hconfig, conv_type, y):\n",
    "    _, xh, xw, _ = x_shape = y.shape\n",
    "    nh, nw = xh, xw\n",
    "    kh, kw, sh, sw, padding = get_shape_params(hconfig, conv_type)\n",
    "    \n",
    "    if padding == 'VALID':\n",
    "        bh, bw = (kh-1)//2, (kw-1)//2\n",
    "        nh, nw = xh - kh + 1, xw - kw + 1\n",
    "        y = y[:, bh:bh+nh, bw:bw+nw:, :]\n",
    "    \n",
    "    if sh != 1 or sw != 1:\n",
    "        bh, bw = (sh-1)//2, (sw-1)//2\n",
    "        mh, mw = nh // sh, nw // sw\n",
    "        y = y[:, bh:bh+mh*sh:sh, bw:bw+mw*sw:sw, :]\n",
    "        \n",
    "    return y, [x_shape, nh, nw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_filter_derv(hconfig, conv_type, G_y, aux):\n",
    "    x_shape, nh, nw = aux\n",
    "    mb_size, xh, xw, chn = x_shape\n",
    "    kh, kw, sh, sw, padding = get_shape_params(hconfig, conv_type)\n",
    "    \n",
    "    if sh != 1 or sw != 1:\n",
    "        bh, bw = (sh-1)//2, (sw-1)//2\n",
    "        mh, mw = nh // sh, nw // sw\n",
    "        G_y_tmp = np.zeros([mb_size, nh, nw, chn], dtype = 'float32')\n",
    "        G_y_tmp[:, bh:bh+mh*sh:sh, bw:bw+mw*sw:sw, :] = G_y\n",
    "        G_y = G_y_tmp\n",
    "        \n",
    "    if padding == 'VALID':\n",
    "        bh, bw = (kh-1)//2, (kw-1)//2\n",
    "        nh, nw = xh - kh + 1, xw - kw + 1\n",
    "        G_y_tmp = np.zeros([mb_size, xh, xw, chn], dtype = 'float32')\n",
    "        G_y_tmp[:, bh:bh+nh, bw:bw+nw:, :] = G_y\n",
    "        G_y = G_y_tmp\n",
    "    \n",
    "    return G_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_params(hconfig, conv_type):\n",
    "    if conv_type:\n",
    "        kh, kw = get_conf_param_2d(hconfig, 'ksize')\n",
    "        sh, sw = get_conf_param_2d(hconfig, 'stride', [1,1])\n",
    "    else:\n",
    "        sh, sw = get_conf_param_2d(hconfig, 'stride', [1,1])\n",
    "        kh, kw = get_conf_param_2d(hconfig, 'ksize', [sh, sw])\n",
    "    padding = get_conf_param(hconfig, 'padding', 'SAME')\n",
    "\n",
    "    return kh, kw, sh, sw, padding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
